{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Data Parsing, Cleansing and Integration\n",
    "## Task 1 and 2\n",
    "#### Student Name: Harold Davies\n",
    "#### Student ID: 3997902\n",
    "\n",
    "Date: 17/04/2024\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* xml.etree.ElementTree\n",
    "\n",
    "## Introduction\n",
    "The goal of task 1 and 2 is to parse an XML file containing 50,703 job listings, clean the data including imputing missing salary information, and export the resulting data frame to csv, keeping a log of all changes made. The XML file was found to have 5 layers, including the root node and the data was parsed into a pandas data frame containing 11 columns and 50,703 rows. The first data cleaning task completed was formatting the missing values, we needed missing values from the Company, ContractType and ContractTime columns to be replaced with \"non-specified\". One of the challenges here were identifying the various data values which corresponded to missing values, and the best approach to this seemed to be filtering through an exported csv version of the data in MSExcel to identify irregular values. Missing values from the Salary column needed to be imputed with numerical values, so missing values in this column were all replaced with None so that they would be easily identifiable later in the project. Salary had many irregularities with values expressed in forms such as # To #, #K or #.# per hour, to name just a few. It was required to use a function leveraging regular expressions to extract the numeric values from each format, calculate consistent annual salary figures and impute these figures back into the salary column in place of the irregularly formatted values. The salary column also had a couple of unrealistic outliers of 10,000,000 with the next higest salary being 150,000 so these were replaced with None values to be replaced later with imputed values. Ten of the unique values in the Location column were found to contain typos, spelling errors or inapprpriate use of case, these were logged and fixed. Id's and Dates needed their data types corrected, but first there was one date identified with a month value over 12 which was changed to 12, apart from that there were no issues with these columns with all the Ids being integer values with 8 digits. Finally, the missing Salary values were imputed using the mean of Salary for the relevant category, and 2 duplicate rows were identified and removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Parsing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Examining and loading data\n",
    "To begin with, in order to examine the data, I searched through the root, child and sub-child levels of the xml file, printing the label of the level, element tag, attributes and text content. However,  I discovered that this was not displaying the entire picture, and I wasn't sure how many layers the xml file had, so I wrote a recursive function to continue drilling down deeper until the the lowest layer has had its details printed. I found the XML had 5 layers, so I labelled them appropriately in my printed output so I could easily examine the shape of the tree structure and its contents. Looking at the printed XML structure, it was not clear to me whether there would be additional child elements besides Source, or whether there were additional SourceNames besides MyUkJobs, so I also wrote a function to check for child nodes with a tag other than Source or a subchild node with tag SourceName and text value besides MyUkJobs. When I ran this I discovered that Source was the only child of JobAds, however there were many different SourceNames which would need to be included when I transformed the format of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save XML file\n",
    "tree = ET.parse('input_file.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to explore and print the structure\n",
    "def explore_element(element, level=0):\n",
    "    #labels for different levels\n",
    "    labels = {0: \"Root\", 1: \"Child\", 2: \"Subchild\", 3: \"Sub-subchild\", 4: \"Sub-sub-subchild\"}\n",
    "    \n",
    "    #tag and attributes with label\n",
    "    print(f\"{labels[level]} Element: {element.tag}\")\n",
    "    for key, value in element.attrib.items():\n",
    "        print(f\"{labels[level]} Attribute - {key}: {value}\")\n",
    "    \n",
    "    #text content if any\n",
    "    if element.text and element.text.strip():\n",
    "        print(f\"{labels[level]} Text Content: {element.text.strip()}\")\n",
    "    \n",
    "    #recursively explore child elements\n",
    "    for child in element:\n",
    "        explore_element(child, level+1)\n",
    "\n",
    "#structure\n",
    "explore_element(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_structure(element, level=0):\n",
    "    #if child element is not \"Source\"\n",
    "    if level == 1 and element.tag != \"Source\":\n",
    "        print(\"Warning: Other child elements present besides 'Source'.\")\n",
    "\n",
    "    #if SourceName is MyUkJobs\n",
    "    if level == 2 and element.tag == \"SourceName\" and element.text.strip() != \"MyUkJobs\":\n",
    "        print(\"Warning: SourceName is not 'MyUkJobs'. It is: \" + element.text)\n",
    "\n",
    "    #recursively explore child elements\n",
    "    for child in element:\n",
    "        check_structure(child, level+1)\n",
    "\n",
    "check_structure(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Parsing data into the required format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I know the shape and contents of the XML, I can extract the data into a list of libraries which will correspond to rows in my new data frame. I am extracting the values corresponding to the sub-subchild level (Id, Title, Location, Company, ContractType, ContractTime, Category and Salary) and sub-sub-subchild level (OpenDate and CloseDate) for every Advertisement subchild, as well as obtaining the SourceName from the parent Source element. When I originally wrote this code, I did not account for potential missing values, and I got errors corresponding to None values in the Company, ContractType, ContractTime and Salary columns, therefore I needed to write if statements for these elements, and will explore the nature of these missing values later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate list for rows\n",
    "data = []\n",
    "#for each advertisement, store the appropriate values in a library and append it to the data list\n",
    "for advertisement in root.findall('.//Advertisement'):\n",
    "    #initiate library for cell values\n",
    "    ad_data = {}\n",
    "    \n",
    "    #extract data for each column\n",
    "    ad_data['Id'] = advertisement.find('.//Id').text\n",
    "    ad_data['Title'] = advertisement.find('.//Title').text\n",
    "    ad_data['Location'] = advertisement.find('.//Location').text\n",
    "\n",
    "    company_element = advertisement.find('.//Company')\n",
    "    if company_element is not None:\n",
    "        ad_data['Company'] = company_element.text\n",
    "    else:\n",
    "        ad_data['Company'] = None\n",
    "\n",
    "    contract_type_element = advertisement.find('.//ContractType')\n",
    "    if contract_type_element is not None:\n",
    "        ad_data['ContractType'] = contract_type_element.text\n",
    "    else:\n",
    "        ad_data['ContractType'] = None\n",
    "    \n",
    "    contract_time_element = advertisement.find('.//ContractTime')\n",
    "    if contract_time_element is not None:\n",
    "        ad_data['ContractTime'] = contract_time_element.text\n",
    "    else:\n",
    "        ad_data['ContractTime'] = None\n",
    "\n",
    "    ad_data['Category'] = advertisement.find('.//Category').text\n",
    "    \n",
    "    salary_element = advertisement.find('.//Salary')\n",
    "    if salary_element is not None:\n",
    "        ad_data['Salary'] = salary_element.text\n",
    "    else:\n",
    "        ad_data['Salary'] = None\n",
    "\n",
    "    date_element = advertisement.find('.//Date')\n",
    "    ad_data['OpenDate'] = date_element.find('.//OpenDate').text\n",
    "    ad_data['CloseDate'] = date_element.find('.//CloseDate').text\n",
    "    \n",
    "    #get SourceName from the parent Source element\n",
    "    source_name = root.find('.//Source/SourceName')\n",
    "    if source_name is not None:\n",
    "        ad_data['SourceName'] = source_name.text\n",
    "    else:\n",
    "        ad_data['SourceName'] = None\n",
    "    \n",
    "    #finally, append the extracted data to the list\n",
    "    data.append(ad_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas data frame\n",
    "df = pd.DataFrame(data, columns=['Id', 'Title', 'Location', 'Company', 'ContractType', 'ContractTime', 'Category', 'Salary', 'OpenDate', 'CloseDate', 'SourceName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Auditing and cleansing the loaded data\n",
    "\n",
    "We will start by exporting the data frame to csv to assist in exploring the data values, and then create a log to track changes made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('raw_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create change log\n",
    "itemlist = ['indexOfdf','ColumnName', 'Orignal', 'Modified', 'ErrorType','Fixing']\n",
    "erlist = pd.DataFrame(columns=itemlist)\n",
    "erlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for updating change log\n",
    "def updateErlist(indexOfdf, ColumnName, Orignal, Modified, ErrorType, Fixing):\n",
    "    #create list from input data\n",
    "    errItem=[indexOfdf,ColumnName, Orignal, Modified, ErrorType,Fixing]\n",
    "    #add list to the end of the error log\n",
    "    erlist.loc[len(erlist)]=errItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate indices to referencxe in the error log\n",
    "indices = df.index \n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four columns contain missing values. There are also other missing values represented by other irregular values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values in the Company column include cells containing \"\", -, N/A and \" \", as well as 2 numerical values 21 and 591. Turns out there is a hospitality and catering company in London called \"21\", but there is no company in London called \"591\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip leading and trailing spaces \n",
    "df['Company'] = df['Company'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask locations of missing values\n",
    "condition = df.Company.isna()\n",
    "#get list of indices of missing values\n",
    "applied_indicies = indices[condition]\n",
    "rows_added = 0\n",
    "#for each missing value, update the change log\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'Company', None, 'non-specified', 'missing value', \"replacing all None with 'non-specified'\")\n",
    "    rows_added += 1\n",
    "print(str(rows_added) + \" rows added to change log\")\n",
    "#change missing values to 'non-specified'\n",
    "df['Company'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Company'] == '-').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"-\" values\n",
    "condition = df.Company == '-'\n",
    "applied_indicies = indices[condition]\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'Company', '-', 'non-specified', 'missing value', \"replacing all '-' with 'non-specified'\")\n",
    "df['Company'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Company'] == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"\" values\n",
    "condition = df.Company == ''\n",
    "applied_indicies = indices[condition]\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'Company', '', 'non-specified', 'missing value', \"replacing all '' with 'non-specified'\")\n",
    "df['Company'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Company'] == \"N/A\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"N/A\" values\n",
    "condition = df.Company == 'N/A'\n",
    "applied_indicies = indices[condition]\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'Company', 'N/A', 'non-specified', 'missing value', \"replacing all 'N/A' with 'non-specified'\")\n",
    "df['Company'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Company'] == \".\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup indices of other irregular values\n",
    "condition = ((df.Company == '.') | (df.Company == '591'))\n",
    "applied_indicies = indices[condition]\n",
    "df[condition]['Company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update change log and fix the values\n",
    "updateErlist(25097, 'Company', '591', 'non-specified', 'typo', \"There is no hospitality and catering company in London called '591'\")\n",
    "df.at[25097, 'Company'] = 'non-specified'\n",
    "updateErlist(42067, 'Company', '.', 'non-specified', 'missing value', \"replacing missing value with 'non-specified'\")\n",
    "df.at[42067, 'Company'] = 'non-specified'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ContractType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values in the Company column also include cells containing None, -, N/A and \" \", we will replace them all with 'non-specified'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ContractType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip leading and trailing spaces\n",
    "df['ContractType'] = df['ContractType'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['ContractType'] == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"\" values\n",
    "condition = df.ContractType == ''\n",
    "applied_indicies = indices[condition]\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'ContractType', '', 'non-specified', 'missing value', \"replacing all '' with 'non-specified'\")\n",
    "df['ContractType'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace None values\n",
    "condition = df.ContractType.isna()\n",
    "applied_indicies = indices[condition]\n",
    "rows_added = 0\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'ContractType', None, 'non-specified', 'missing value', \"replacing all None with 'non-specified'\")\n",
    "    rows_added += 1\n",
    "print(str(rows_added) + \" rows added to change log\")\n",
    "df['ContractType'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['ContractType'] == '-').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"-\" values\n",
    "condition = df.ContractType == '-'\n",
    "applied_indicies = indices[condition]\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'ContractType', '-', 'non-specified', 'missing value', \"replacing all '-' with 'non-specified'\")\n",
    "df['ContractType'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['ContractType'] == \"N/A\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"N/A\" values\n",
    "condition = df.ContractType == 'N/A'\n",
    "applied_indicies = indices[condition]\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'ContractType', 'N/A', 'non-specified', 'missing value', \"replacing all 'N/A' with 'non-specified'\")\n",
    "df['ContractType'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ContractTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values in the ContractTime column also include cells containing None, -, N/A and \" \", again, we will replace them with 'non-specified'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ContractTime'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip leading and trailing spaces\n",
    "df['ContractTime'] = df['ContractTime'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace None values\n",
    "condition = df.ContractTime.isna()\n",
    "applied_indicies = indices[condition]\n",
    "rows_added = 0\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'ContractTime', None, 'non-specified', 'missing value', \"replacing all None with 'non-specified'\")\n",
    "    rows_added += 1\n",
    "print(str(rows_added) + \" rows added to change log\")\n",
    "df['ContractTime'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['ContractTime'] == '-').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"-\" values\n",
    "condition = df.ContractTime == '-'\n",
    "applied_indicies = indices[condition]\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'ContractTime', '-', 'non-specified', 'missing value', \"replacing all '-' with 'non-specified'\")\n",
    "df['ContractTime'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['ContractTime'] == \"N/A\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"N/A\" values\n",
    "condition = df.ContractTime == 'N/A'\n",
    "applied_indicies = indices[condition]\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'ContractTime', 'N/A', 'non-specified', 'missing value', \"replacing all 'N/A' with 'non-specified'\")\n",
    "df['ContractTime'][condition] = 'non-specified'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values in the Salary column include \" \", \"-\", None and N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Salary'] == \" \").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Salary'] == \"-\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Salary'].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Salary'] == \"N/A\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Salary'] == \"\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \" \", \"-\" and \"N/A\" values\n",
    "condition = (df.Salary == ' ') | (df.Salary == '-') | (df.Salary == 'N/A')\n",
    "applied_indicies = indices[condition]\n",
    "for ind in applied_indicies:\n",
    "    updateErlist(ind, 'Salary', 'N/A or - or \" \"', None, 'missing value', \"replacing all 'N/A' with 'non-specified'\")\n",
    "df['Salary'][condition] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irregularities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********I'm up to here proof reading, commenting and report writing - need to address Salary Outliers\n",
    "\n",
    "\n",
    "The salary column contains irregularities in the form of different variations of text and numbers to represent the job salary. As seen below, variations include # per Annum, # To #, # - #, #/Year, #K, #.# per hour and #.# p/h. I will calculate averages for # To #, and change all values to a floating point number representing the annual salary for the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find where there are non-None and non-numeric values in the 'Salary' column\n",
    "alphabetic_mask = df['Salary'].apply(lambda x: isinstance(x, str) and any(c.isalpha() or c == '-' for c in x))\n",
    "\n",
    "#get the relevant rows\n",
    "values_with_alphabetic = df[alphabetic_mask]\n",
    "\n",
    "#remove leading numbers to reduce the # of unique values to a manageable level\n",
    "values_with_alphabetic['Salary'] = values_with_alphabetic['Salary'].str.replace(r'^\\d+', '', regex=True)\n",
    "\n",
    "#show unique non-None and non-numeric values\n",
    "values_with_alphabetic['Salary'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to deal with all cases\n",
    "def normalised(sal_string):\n",
    "    #num To num\n",
    "    result = re.fullmatch(r'(\\d+) To (\\d+)', sal_string)\n",
    "    if result:\n",
    "        salary = (float(result.group(1))+float(result.group(2)))/2\n",
    "        #print(\"found # To #: \" + str(result[0]) + \" converting to: \" + str(salary))\n",
    "        return salary\n",
    "    #num - num\n",
    "    result = re.fullmatch(r'(\\d+) - (\\d+)', sal_string)\n",
    "    if result:\n",
    "        salary = (float(result.group(1))+float(result.group(2)))/2\n",
    "        #print(\"found # - #: \" + str(result[0]) + \" converting to: \" + str(salary))\n",
    "        return (float(result.group(1))+float(result.group(2)))/2\n",
    "    #num( per Annum /Year)\n",
    "    result = re.fullmatch(r'(\\d+)(( per Annum)|(/Year))', sal_string)\n",
    "    if result:\n",
    "        #print(\"found # per Annum or #/Year: \" + str(result[0]) + \" converting to: \" + str(result.group(1)))\n",
    "        return float(result.group(1))\n",
    "    #numK\n",
    "    result = re.fullmatch(r'(\\d+)K', sal_string)\n",
    "    if result:\n",
    "        #print(\"found #K: \" + str(result[0]) + \" converting to: \" + str(float(result.group(1))*1000))\n",
    "        return float(result.group(1))*1000\n",
    "    #num(.# per hour .# p/h)\n",
    "    result = re.fullmatch(r'(\\d+\\.?\\d+)( per hour| p/h)', sal_string)\n",
    "    if result:\n",
    "        #print(\"found #.# per hour or #.# p/h: \" + str(result[0]) + \" converting to: \" + str(float(result.group(1))*37.5*52))\n",
    "        return float(result.group(1))*37.5*52\n",
    "    #return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a new column using the function and rounding to 2 decimal places\n",
    "df['Salary_fixed'] = round(df['Salary'].astype(str).apply(lambda x: normalised(x)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed salary values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mask of fixed salary values\n",
    "condition = pd.notna(df['Salary_fixed'])\n",
    "#get list of indices where salaries need to be fixed\n",
    "applied_indices = indices[condition]\n",
    "#initiate list for updates\n",
    "update_list = []\n",
    "#for each index, update the change log and create a tuple to update df\n",
    "for ind in applied_indices:\n",
    "    original_salary = df['Salary'].iloc[ind]\n",
    "    modified_salary = df['Salary_fixed'].iloc[ind]\n",
    "    updateErlist(ind, 'Salary', original_salary, modified_salary, 'inconsistent formatting', \"replacing inconsistently formatted salary with floating point number\")\n",
    "    update_tuple = (ind, 'Salary', modified_salary)\n",
    "    update_list.append(update_tuple)\n",
    "\n",
    "#update df\n",
    "for ind, col, new_value in update_list:\n",
    "    df.at[ind, col] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The salary column contains a couple of values equal to 10,000,000 which are quite clearly incorrect with the next highest value being 150,000. I will change these values to None so that they will be imputed with estimated values later in the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print salary values over 100k\n",
    "for value in df['Salary']:\n",
    "    try:\n",
    "        value = float(value)\n",
    "        if value > 100000:\n",
    "            print(value)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get indices of outliers\n",
    "df[df['Salary'] == '10000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update change log and change values to None\n",
    "updateErlist(40787, 'Salary', '10000000', None, 'outlier', \"replacing extreme/unrealistic outlier with None value to be imputed later\")\n",
    "updateErlist(49589, 'Salary', '10000000', None, 'outlier', \"replacing extreme/unrealistic outlier with None value to be imputed later\")\n",
    "df.at[40787, 'Salary'] = None\n",
    "df.at[49589, 'Salary'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typos and Spelling Mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the tail of the valuecounts shows the infrequently entered typos and mistakes\n",
    "df['Location'].value_counts().tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the indices of the errors\n",
    "condition = ((df.Location == 'Manchaster') | (df.Location == 'BRISTOL') | (df.Location == 'london') | (df.Location == 'ABERDEEN') | (df.Location == 'Livepool') \n",
    "             | (df.Location == 'Oxfords') | (df.Location == 'Leads') | (df.Location == 'birmingham') | (df.Location == 'Cembridge') | (df.Location == 'HAMpshire'))\n",
    "applied_indicies = indices[condition]\n",
    "df[condition]['Location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each error, update the log and fix the value\n",
    "updateErlist(544, 'Location', 'ABERDEEN', 'Aberdeen', 'typo', \"replacing with correct case letters\")\n",
    "df.at[544, 'Location'] = 'Aberdeen'\n",
    "updateErlist(3132, 'Location', 'BRISTOL', 'Bristol', 'typo', \"replacing with correct case letters\")\n",
    "df.at[3132, 'Location'] = 'Bristol'\n",
    "updateErlist(21121, 'Location', 'london', 'London', 'typo', \"replacing with correct case letters\")\n",
    "df.at[21121, 'Location'] = 'London'\n",
    "updateErlist(36396, 'Location', 'Manchaster', 'Manchester', 'typo', \"fixing typo\")\n",
    "df.at[36396, 'Location'] = 'Manchester'\n",
    "updateErlist(47528, 'Location', 'Livepool', 'Liverpool', 'typo', \"fixing typo\")\n",
    "df.at[47528, 'Location'] = 'Liverpool'\n",
    "updateErlist(2664, 'Location', 'birmingham', 'Birmingham', 'typo', \"replacing with correct case letters\")\n",
    "df.at[2664, 'Location'] = 'Birmingham'\n",
    "updateErlist(4048, 'Location', 'Leads', 'Leeds', 'typo', \"replacing with correct case letters\")\n",
    "df.at[4048, 'Location'] = 'Leeds'\n",
    "updateErlist(7604, 'Location', 'Cembridge', 'Cambridge', 'typo', \"replacing with correct case letters\")\n",
    "df.at[7604, 'Location'] = 'Cambridge'\n",
    "updateErlist(19470, 'Location', 'HAMpshire', 'Hampshire', 'typo', \"replacing with correct case letters\")\n",
    "df.at[19470, 'Location'] = 'Hampshire'\n",
    "updateErlist(21941, 'Location', 'HAMpshire', 'Hampshire', 'typo', \"replacing with correct case letters\")\n",
    "df.at[21941, 'Location'] = 'Hampshire'\n",
    "updateErlist(41739, 'Location', 'Cembridge', 'Cambridge', 'typo', \"replacing with correct case letters\")\n",
    "df.at[41739, 'Location'] = 'Cambridge'\n",
    "updateErlist(42678, 'Location', 'Oxfords', 'Oxford', 'typo', \"replacing with correct case letters\")\n",
    "df.at[42678, 'Location'] = 'Oxford'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the tail to see the edited values now fall into bigger value counts\n",
    "df['Location'].value_counts().tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check category value counts - all good\n",
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change Ids to int type\n",
    "df['Id'] = df['Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check max and min to ensure all Ids are 8 digits\n",
    "print(\"Id min = \" + str(df['Id'].min()) + \" and Id max = \" + str(df['Id'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon attempting to use the to_datetime pandas function to fix the date formats, I discovered there was at least one date not conforming to the observed original format. I use a function to detect any such case and then fix this before going ahead and transforming the format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define finction to find dates with month > 12\n",
    "def find_inconsistent_dates(date_string):\n",
    "    #months > 12\n",
    "    result = re.fullmatch(r'\\d{4}(1[3-9]|[23]\\d).*', date_string)\n",
    "    if result:\n",
    "        print(\"found inconsistent date format: \" + date_string)\n",
    "        return date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute function on all OpenDate values\n",
    "for date in df['OpenDate']:\n",
    "    find_inconsistent_dates(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find index of troublesome value\n",
    "df[df['OpenDate'] == '20121612T150000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update change log and fix value\n",
    "updateErlist(2615, 'OpenDate', '20121612T150000', '20121212T150000', 'violation of integrity constraint', \"changing month to 12 instead of 16 (impossible value for month)\")\n",
    "df.at[2615, 'OpenDate'] = '20121212T150000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check CloseDate values\n",
    "for date in df['CloseDate']:\n",
    "    find_inconsistent_dates(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranform formats\n",
    "df['OpenDate'] = pd.to_datetime(df['OpenDate'], format='%Y%m%dT%H%M%S')\n",
    "df['CloseDate'] = pd.to_datetime(df['OpenDate'], format='%Y%m%dT%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data types - Salary should be the final column remaining to be fixed\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rows where Salary is not None\n",
    "df_salary = df[df['Salary'] != None]\n",
    "\n",
    "#convert the salary column to floating point numbers\n",
    "df_salary['Salary'] = df_salary['Salary'].astype(float)\n",
    "\n",
    "#calculate and store the mean salary value for each Category\n",
    "category_salary_mean = df_salary.groupby('Category')['Salary'].mean(numeric_only=True)\n",
    "\n",
    "#merge values onto df\n",
    "df = df.merge(category_salary_mean, left_on='Category', right_index=True, suffixes=('', '_mean_for_category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through rows where the salary value is missing, updating the change log and imputing the relevant value\n",
    "for index, row in df[df['Salary'].isna()].iterrows():\n",
    "    original_value = None\n",
    "    modified_value = row['Salary_mean_for_category']\n",
    "    error_type = 'missing value'\n",
    "    fixing = \"imputing missing value with mean salary for category\"\n",
    "    updateErlist(index, 'Salary', original_value, modified_value, error_type, fixing)\n",
    "    #impute the 'Salary' column with the mean salary for the corresponding category\n",
    "    df.at[index, 'Salary'] = modified_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop ancillary columns which are no longer needed\n",
    "df.drop(['Salary_fixed', 'Salary_mean_for_category'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "df[df.duplicated(subset=df.columns.difference(['Id']), keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update change log\n",
    "updateErlist(43706, 'All', df.iloc[43706], 'deleted', 'duplicate row', 'deleted duplicate row')\n",
    "updateErlist(47601, 'All', df.iloc[47601], 'deleted', 'duplicate row', 'deleted duplicate row')\n",
    "#delete duplicate rows\n",
    "df = df.drop(index=[47601, 43706])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "\n",
    "The data has been parsed and cleaned, with missing values dealt with as per the goal of the project, we will export them to csv for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#export cleaned csv\n",
    "df.to_csv('clean_data.csv', index=False)\n",
    "#export change log\n",
    "erlist.to_csv('change_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "After the data was parsed, many issues needed to be addressed in order to thoroughly clean the data. Four columns needed missing values identified and reformatted, salary had a couple outliers to fix, location had numerous typos, spelling errors and inconsistent use of case, opendate violated an integrity constraint, dates and Ids needed data types adjusted, Salary required calculation and imputation of values to replace missing values and finally there were 2 duplicate rows. The data has now been well cleaned and a change log and the cleaned dataset have been exported. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
