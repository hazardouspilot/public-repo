{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "df = pd.read_csv(\"name_of_data_file.csv\")\n",
    "\n",
    "#sample\n",
    "df_sample = df.sample(n=5000, random_state=999) \n",
    "# use \".reset_index(drop=True)\" & same random_state if sampling features and targets seperately\n",
    "\n",
    "#split into features and target\n",
    "Data = df.drop(columns = 'diagnosis').values\n",
    "target = df['diagnosis']\n",
    "\n",
    "#check target feature encoding\n",
    "np.unique(target, return_counts = True)\n",
    "\n",
    "#transform target to be 1s and 0s if necessary - first label alphabetically will be 0\n",
    "from sklearn import preprocessing\n",
    "target = preprocessing.LabelEncoder().fit_transform(target)\n",
    "\n",
    "#swap 1s and 0s if necesssary\n",
    "target = np.where(target==0, 1, 0)\n",
    "\n",
    "#scale features\n",
    "Data = preprocessing.MinMaxScaler().fit_transform(Data)\n",
    "\n",
    "#train-test split - stratify ensures train & test have consistent proportions of target feature\n",
    "from sklearn.model_selection import train_test_split\n",
    "D_train, D_test, t_train, t_test = train_test_split(Data, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    stratify=target,\n",
    "                                                    shuffle=True, #default setting\n",
    "                                                    random_state=999)\n",
    "\n",
    "#verify proportions of target features in train vs test sets\n",
    "sum(t_train==1)/len(t_train)    #prop binary target feature in training set\n",
    "sum(t_test==1)/len(t_test)      #prop binary target feature in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, p=2) #euclidean distance\n",
    "knn_classifier.fit(D_train, t_train)\n",
    "knn_classifier.score(D_test, t_test) #output's the accuracy, use a variable if need to save it\n",
    "\n",
    "#predict and get accuracy\n",
    "pred_test = knn_classifier.predict(D_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(pred_test, t_test) #output's the score, use a variable if need to save it\n",
    "#KNeighborsClassifier?\n",
    "\n",
    "#DT Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=999)\n",
    "dt_classifier.fit(D_train, t_train)\n",
    "dt_classifier.score(D_test, t_test) #output's the score, use a variable if need to save it\n",
    "#DecisionTreeClassifier?\n",
    "\n",
    "#DT Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt_regressor = DecisionTreeRegressor(max_depth = 4, random_state = 999)\n",
    "\n",
    "#evaluation for continuous variable\n",
    "dt_regressor.fit(D_train, t_train)\n",
    "t_pred = dt_regressor.predict(D_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(t_test, t_pred)            #MSE\n",
    "rmse = np.sqrt(mse)                                 #RMSE\n",
    "print(f'RMSE_DT_Regressor: {rmse:.2f}')\n",
    "\n",
    "#RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=999)\n",
    "#.fit and .score as above\n",
    "#RandomForestClassifier?\n",
    "\n",
    "#RF Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100)\n",
    "#RandomForestRegressor?\n",
    "\n",
    "#GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_classifier = GaussianNB(var_smoothing=10**(-3))\n",
    "#GaussianNB?\n",
    "\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "svm_classifier = SVC()\n",
    "#SVC?\n",
    "\n",
    "#linear regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_regressor = LinearRegression()\n",
    "#LinearRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict using fitted model on unseen data\n",
    "new_obs = Data[0:3] # supposed to be new data (but we dont have any)\n",
    "knn_classifier.predict(new_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple hyperparameter comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare accuracies with different parameters\n",
    "k_list = list([1, 3, 5, 10, 15, 20, 25])\n",
    "\n",
    "knn_score_manhattan = []\n",
    "knn_score_euclidean = []\n",
    "\n",
    "for k in k_list:\n",
    "    knn_classifier_manhattan = KNeighborsClassifier(n_neighbors=k, p=1)\n",
    "    knn_classifier_manhattan.fit(D_train, t_train)\n",
    "    knn_score_manhattan.append([knn_classifier_manhattan.score(D_test, t_test)])\n",
    "    \n",
    "    knn_classifier_euclidean = KNeighborsClassifier(n_neighbors=k, p=2)\n",
    "    knn_classifier_euclidean.fit(D_train, t_train)\n",
    "    knn_score_euclidean.append([knn_classifier_euclidean.score(D_test, t_test)])\n",
    "\n",
    "results = pd.DataFrame({'manhattan': knn_score_manhattan, \n",
    "                        'euclidean': knn_score_euclidean,\n",
    "                        'k': k_list})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full feature set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of full feature set using DT wrapper\n",
    "clf = DecisionTreeClassifier(random_state=999)      #Wrapper\n",
    "\n",
    "#repeated stratified k fold cross validation\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, \n",
    "                                     n_repeats=3,\n",
    "                                     random_state=999)\n",
    "scoring_metric = 'accuracy'\n",
    "#scoring_metric = 'roc_auc'             #example alternative\n",
    "cv_results_full = cross_val_score(  estimator=clf,\n",
    "                                    X=Data,\n",
    "                                    y=target, \n",
    "                                    cv=cv_method, \n",
    "                                    scoring=scoring_metric)\n",
    "cv_results_full.mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate cv_method\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "cv_method = StratifiedKFold(n_splits=5, shuffle=True, random_state=999)\n",
    "#StratifiedKFold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "def plot_imp(best_features, scores, method_name):   \n",
    "    plt.barh(best_features, scores)\n",
    "    plt.title(method_name + ' Feature Importances')\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 1\n",
    "\n",
    "#F-Score\n",
    "from sklearn import feature_selection as fs\n",
    "fs_fit_fscore = fs.SelectKBest(fs.f_classif, k=num_features)\n",
    "fs_fit_fscore.fit_transform(Data, target)\n",
    "fs_indices_fscore = np.argsort(np.nan_to_num(fs_fit_fscore.scores_))[::-1][0:num_features]\n",
    "fs_indices_fscore                                                       #feature indices\n",
    "best_features_fscore = df.columns[fs_indices_fscore].values\n",
    "best_features_fscore                                                    #features\n",
    "feature_importances_fscore = fs_fit_fscore.scores_[fs_indices_fscore]\n",
    "feature_importances_fscore                                              #feature importances\n",
    "plot_imp(best_features_fscore, feature_importances_fscore, 'F-Score')   #plot of importances\n",
    "\n",
    "cv_results_fscore = cross_val_score(estimator=clf,\n",
    "                             X=Data[:, fs_indices_fscore],\n",
    "                             y=target, \n",
    "                             cv=cv_method, \n",
    "                             scoring=scoring_metric)\n",
    "cv_results_fscore.mean().round(3)                                       #score of features\n",
    "\n",
    "#Mutual Information\n",
    "fs_fit_mutual_info = fs.SelectKBest(fs.mutual_info_classif, k=num_features)\n",
    "fs_fit_mutual_info.fit_transform(Data, target)\n",
    "fs_indices_mutual_info = np.argsort(fs_fit_mutual_info.scores_)[::-1][0:num_features]      #np.nan_to_num may be helpful here to deal with null values?\n",
    "fs_indices_mutual_info                                                  #feature indices\n",
    "best_features_mutual_info = df.columns[fs_indices_mutual_info].values\n",
    "best_features_mutual_info                                               #features\n",
    "feature_importances_mutual_info = fs_fit_mutual_info.scores_[fs_indices_mutual_info]\n",
    "feature_importances_mutual_info                                         #feature importances\n",
    "plot_imp(best_features_mutual_info, feature_importances_mutual_info, 'Mutual Information')\n",
    "\n",
    "cv_results_mutual_info = cross_val_score(estimator=clf,\n",
    "                             X=Data[:, fs_indices_mutual_info],\n",
    "                             y=target, \n",
    "                             cv=cv_method, \n",
    "                             scoring=scoring_metric)\n",
    "cv_results_mutual_info.mean().round(3)                                  #score of features\n",
    "\n",
    "#Random Forest Importance\n",
    "model_rfi = RandomForestClassifier(n_estimators=100)\n",
    "model_rfi.fit(Data, target)\n",
    "fs_indices_rfi = np.argsort(model_rfi.feature_importances_)[::-1][0:num_features]   #np.nan_to_num may be helpful here to deal with null values?\n",
    "fs_indices_rfi                                                          #feature indices\n",
    "best_features_rfi = df.columns[fs_indices_rfi].values\n",
    "best_features_rfi                                                       #features\n",
    "feature_importances_rfi = model_rfi.feature_importances_[fs_indices_rfi]\n",
    "feature_importances_rfi                                                 #feature importances\n",
    "plot_imp(best_features_rfi, feature_importances_rfi, 'Random Forest')   #plot of importances\n",
    "\n",
    "cv_results_rfi = cross_val_score(estimator=clf,\n",
    "                             X=Data[:, fs_indices_rfi],\n",
    "                             y=target, \n",
    "                             cv=cv_method, \n",
    "                             scoring=scoring_metric)\n",
    "cv_results_rfi.mean().round(3)                                          #score of features\n",
    "\n",
    "#spFSR\n",
    "from spFSR import SpFSR                 #must have spFSR.py in working directory\n",
    "# pred_type needs to be 'c' for classification and 'r' for regression datasets\n",
    "sp_engine = SpFSR(x=Data, y=target, pred_type='c', wrapper=clf, scoring='accuracy')\n",
    "sp_output = sp_engine.run(num_features=num_features).results\n",
    "fs_indices_spfsr = sp_output.get('selected_features')\n",
    "fs_indices_spfsr                                                        #feature indices\n",
    "best_features_spfsr = df.columns[fs_indices_spfsr]\n",
    "best_features_spfsr                                                     #features\n",
    "feature_importances_spfsr = sp_output.get('selected_ft_importance')\n",
    "feature_importances_spfsr                                               #feature importances\n",
    "plot_imp(best_features_spfsr, feature_importances_spfsr, 'spFSR')       #plot of importances\n",
    "\n",
    "cv_results_spfsr = cross_val_score(estimator=clf,\n",
    "                             X=Data[:, fs_indices_spfsr],\n",
    "                             y=target, \n",
    "                             cv=cv_method, \n",
    "                             scoring=scoring_metric)\n",
    "cv_results_spfsr.mean().round(3)                                        #score of features\n",
    "\n",
    "print('Full Set of Features:', cv_results_full.mean().round(3))\n",
    "print('F-Score:', cv_results_fscore.mean().round(3))\n",
    "print('Mutual Information:', cv_results_mutual_info.mean().round(3))\n",
    "print('RFI:', cv_results_rfi.mean().round(3))\n",
    "print('spFSR:', cv_results_spfsr.mean().round(3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-testing spFSR against other feature set results\n",
    "from scipy import stats\n",
    "print(stats.ttest_rel(cv_results_spfsr, cv_results_fscore).pvalue.round(3))\n",
    "print(stats.ttest_rel(cv_results_spfsr, cv_results_mutual_info).pvalue.round(3))\n",
    "print(stats.ttest_rel(cv_results_spfsr, cv_results_rfi).pvalue.round(3))\n",
    "print(stats.ttest_rel(cv_results_spfsr, cv_results_full).pvalue.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib as plt\n",
    "\n",
    "#hypothetical data prep\n",
    "df = pd.read_csv()\n",
    "Data, target = df.data, df.target\n",
    "Data = preprocessing.MinMaxScaler().fit_transform(Data)\n",
    "target = np.where(target==0, 1, 0)\n",
    "D_train, D_test, t_train, t_test = train_test_split(Data, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Modeling and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypothetical model tuning\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, \n",
    "                                    n_repeats=3, \n",
    "                                    random_state=8)\n",
    "model_KNN = KNeighborsClassifier()\n",
    "params_KNN = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7], \n",
    "              'p': [1, 2, 5]}\n",
    "scoring_metrics = ['accuracy', 'recall', 'roc_auc', 'neg_mean_squared_error', ]\n",
    "gs_KNN = GridSearchCV(estimator=model_KNN, \n",
    "                      param_grid=params_KNN, \n",
    "                      cv=cv_method,\n",
    "                      verbose=1, \n",
    "                      scoring='accuracy',\n",
    "                      return_train_score=True)\n",
    "gs_KNN.fit(D_train, t_train)\n",
    "t_pred = gs_KNN.predict(D_test)\n",
    "gs_KNN.best_params_\n",
    "gs_KNN.best_estimator_\n",
    "gs_KNN.best_score_\n",
    "gs_KNN.cv_results_['mean_test_score']\n",
    "#create data frame of results for visualisation\n",
    "results_KNN = pd.DataFrame(gs_KNN.cv_results_['params'])\n",
    "results_KNN['test_score'] = gs_KNN.cv_results_['mean_test_score']\n",
    "results_KNN['metric'] = results_KNN['p'].replace([1,2,5], [\"Manhattan\", \"Euclidean\", \"Minkowski\"])\n",
    "#draw a line for each distance metric\n",
    "for i in [\"Manhattan\", \"Euclidean\", \"Minkowski\"]:\n",
    "    temp = results_KNN[results_KNN['metric'] == i]\n",
    "    plt.plot(temp['n_neighbors'], temp['test_score'], marker = '.', label = i)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbours')\n",
    "plt.ylabel(\"Mean CV Score\")\n",
    "plt.title(\"KNN Performance Comparison\")\n",
    "plt.show()\n",
    "\n",
    "#DT visualisation Example\n",
    "results_DT = pd.DataFrame(gs_DT.cv_results_['params'])\n",
    "results_DT['test_score'] = gs_DT.cv_results_['mean_test_score']\n",
    "results_DT.columns\n",
    "for i in ['gini', 'entropy']:\n",
    "    temp = results_DT[results_DT['criterion'] == i]\n",
    "    temp_average = temp.groupby('max_depth').agg({'test_score': 'mean'})\n",
    "    plt.plot(temp_average, marker = '.', label = i)\n",
    "plt.legend()\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel(\"Mean CV Score\")\n",
    "plt.title(\"DT Performance Comparison\")\n",
    "plt.show()\n",
    "\n",
    "#G-NB model tuning and visualisation example\n",
    "#tuning\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "np.random.seed(999)\n",
    "nb_classifier = GaussianNB()\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
    "                     param_grid=params_NB, \n",
    "                     cv=cv_method,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy')\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "Data_transformed = PowerTransformer().fit_transform(Data)\n",
    "gs_NB.fit(Data_transformed, target)\n",
    "gs_NB.best_params_\n",
    "gs_NB.best_score_\n",
    "#visualisation\n",
    "results_NB = pd.DataFrame(gs_NB.cv_results_['params'])\n",
    "results_NB['test_score'] = gs_NB.cv_results_['mean_test_score']\n",
    "plt.plot(results_NB['var_smoothing'], results_NB['test_score'], marker = '.')    \n",
    "plt.xlabel('Var. Smoothing')\n",
    "plt.ylabel(\"Mean CV Score\")\n",
    "plt.title(\"NB Performance Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary classification evaluation\n",
    "from sklearn import metrics\n",
    "#accuracy\n",
    "metrics.accuracy_score(y_true=t_test, y_pred=t_pred)        #error rate is 1 - accuracy\n",
    "#precision\n",
    "metrics.precision_score(t_test, t_pred)\n",
    "#recall\n",
    "metrics.recall_score(t_test, t_pred)\n",
    "#f1\n",
    "metrics.f1_score(t_test, t_pred)\n",
    "#AUC\n",
    "metrics.roc_auc_score(t_test, t_pred)\n",
    "#confusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(t_test, t_pred) #works for multinomial predicitons\n",
    "#via heatmap\n",
    "import seaborn as sns\n",
    "cf_matrix = metrics.confusion_matrix(t_test, t_pred)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='g')\n",
    "ax.set(xlabel='Predicted values', ylabel='Actual values')\n",
    "ax.set_title(\"Confusion matrix on test set\")\n",
    "plt.show()\n",
    "#classification report\n",
    "print(metrics.classification_report(t_test, t_pred))        #works for multinomial predicitons, produces macro results which doesnt account for class imbalance\n",
    "#profit matrix\n",
    "profit_matrix = np.array([[0, -10], [-50, 100]])\n",
    "overall_profit_matrix = profit_matrix*confusion_matrix\n",
    "net_profit_or_loss = np.sum(overall_profit_matrix)\n",
    "#ROC curve visualisation\n",
    "t_prob = gs_KNN.predict_proba(D_test)\n",
    "fpr, tpr, _ = metrics.roc_curve(t_test, t_prob[:, 1])\n",
    "df = pd.DataFrame({'fpr': fpr, 'tpr': tpr})\n",
    "import matplotlib.pyplot as plt\n",
    "ax = df.plot.line(x='fpr', y='tpr', title='ROC Curve', legend=False, marker = '.')\n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "ax.set_xlabel(\"False Postive Rate (FPR)\")\n",
    "ax.set_ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.show();\n",
    "#shifting the prediction threshold\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(t_test, t_prob[:,1], pos_label=1)\n",
    "accuracy = [metrics.accuracy_score(t_test, t_prob[:,1]>=t) for t in thresholds]\n",
    "results = pd.DataFrame({\"Precision\": precision[1:], \n",
    "                        \"Recall\": recall[1:], \n",
    "                        \"Threshold\": thresholds,\n",
    "                        \"Accuracy\": accuracy})\n",
    "#precision vs threshold\n",
    "ax = results.plot.line(x='Threshold', y='Precision', title='Precision vs threshold', legend=False, marker = '.')\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "plt.show()\n",
    "#recall vs threshold\n",
    "ax = results.plot.line(x='Threshold', y='Recall', title='Recall vs threshold', legend=False, marker = '.')\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Recall\")\n",
    "plt.show();\n",
    "#accuracy vs threshold\n",
    "ax = results.plot.line(x='Threshold',y=\"Accuracy\", title='Accuracy vs threshold', legend=False, marker = '.')\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomioal Classification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multinomial classification metrics - micro=global metircs, macro=label only metrics\n",
    "metrics.recall_score(t_test, t_pred, average='micro')\n",
    "metrics.precision_score(t_test, t_pred, average='micro')\n",
    "metrics.f1_score(t_test, t_pred, average='micro')\n",
    "metrics.balanced_accuracy_score(t_test, t_pred) #class accuracy using arithmetic mean, works for binary or multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regresors - (and a pipeline)\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import tree\n",
    "models = {'KNN': KNeighborsRegressor(),\n",
    "          'DT': tree.DecisionTreeRegressor()}\n",
    "models_parameters = {'KNN': {'n_neighbors': [1, 2, 3, 4, 5], \n",
    "                             'p': [1, 2, 3]},\n",
    "                     'DT': {'max_depth': [2, 3, 4], \n",
    "                            'min_samples_split': [2, 3, 4, 5]}}\n",
    "fitted_models = {} # this creates an empty dictionary\n",
    "#fit models using gridsearch\n",
    "for m in models: # this will loop over the dictionary keys\n",
    "    print(f'\\nHyperparameter tuning for {m}:')\n",
    "    gs = GridSearchCV(estimator=models[m], \n",
    "                      param_grid=models_parameters[m], \n",
    "                      cv=cv_method,\n",
    "                      verbose=1, \n",
    "                      scoring='neg_mean_squared_error')\n",
    "    gs.fit(D_train, t_train)\n",
    "    fitted_models[m] = gs\n",
    "    print(f'Best {m} model: {gs.best_params_}')\n",
    "#evaluate\n",
    "from sklearn import metrics\n",
    "for m in fitted_models:\n",
    "    t_pred = fitted_models[m].predict(D_test)\n",
    "    mse = metrics.mean_squared_error(t_test, t_pred)        #MSE\n",
    "    me = np.sqrt(metrics.mean_squared_error(t_test, t_pred))     #mean error?\n",
    "    print(f'MSE of {m} is: {mse}')\n",
    "    from sklearn import metrics\n",
    "for m in fitted_models:\n",
    "    t_pred = fitted_models[m].predict(D_test)\n",
    "    mae = metrics.mean_absolute_error(t_test, t_pred)       #MAE\n",
    "    r2 = metrics.r2_score(t_test, t_pred)                   #R_squared\n",
    "    print(f'MAE and r-squared {m} are: {mae}, {r2}')\n",
    "#visualising residuals\n",
    "t_pred_knn = fitted_models['KNN'].predict(D_test)\n",
    "residuals_knn = t_test - t_pred_knn\n",
    "plt.hist(residuals_knn,20)\n",
    "plt.xlabel(\"Residuals (Binned)\")\n",
    "plt.ylabel(\"Number of Residuals\")\n",
    "plt.title(\"Distribution of Residuals for KNN Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK5, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = load_breast_cancer()\n",
    "Data_unscaled, target = df.data, df.target\n",
    "data_scaler = preprocessing.MinMaxScaler().fit(Data_unscaled)       #scaler model\n",
    "Data = data_scaler.transform(Data_unscaled)\n",
    "target = np.where(target==0, 1, 0)                   #reverse the labels if necesary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=999)\n",
    "\n",
    "pipe_KNN = Pipeline([('fselector', SelectKBest()), \n",
    "                     ('knn', KNeighborsClassifier())])\n",
    "params_pipe_KNN = {'fselector__score_func': [f_classif, mutual_info_classif],\n",
    "                   'fselector__k': [10, 20, Data.shape[1]],\n",
    "                   'knn__n_neighbors': [1, 2, 3, 4, 5],\n",
    "                   'knn__p': [1, 2]}\n",
    "gs_pipe_KNN = GridSearchCV(estimator=pipe_KNN, \n",
    "                           param_grid=params_pipe_KNN, \n",
    "                           cv=cv_method,\n",
    "                           #parallel processing:\n",
    "                           n_jobs=-2,                   #use all but 1 core\n",
    "                           scoring='roc_auc',\n",
    "                           verbose=1) \n",
    "gs_pipe_KNN.fit(Data, target);\n",
    "gs_pipe_KNN.best_params_            #.best_score_   .best_estimator_\n",
    "\n",
    "#randomized grid search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_iter_search = 10\n",
    "random_pipe_KNN = RandomizedSearchCV(estimator=pipe_KNN, \n",
    "                           param_distributions=params_pipe_KNN, \n",
    "                           cv=cv_method,\n",
    "                           scoring='roc_auc',\n",
    "                           n_iter=n_iter_search,         #only use n parameter combos\n",
    "                           verbose=1) \n",
    "random_pipe_KNN.fit(Data, target);\n",
    "#can get best params, score and estimator as usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving and loading models\n",
    "import joblib\n",
    "joblib.dump(gs_pipe_KNN.best_estimator_, 'best_KNN.pkl', compress = 1)\n",
    "saved_knn = joblib.load('best_KNN.pkl')\n",
    "saved_knn.predict(Data[0:20,])                  #predict on new data\n",
    "\n",
    "#t-test comparable CV result sets\n",
    "from scipy import stats                         #use full CV result sets\n",
    "print(stats.ttest_rel(cv_results_DT, cv_results_KNN).pvalue.round(3))\n",
    "\n",
    "#model deployment\n",
    "knn_classifier_deployment = gs_pipe_KNN.best_estimator_\n",
    "knn_classifier_deployment.fit(Data, target)             #fit to entire dataset\n",
    "obs_for_prediction_unscaled = Data_unscaled[0:3, ]      #hypothetical new data\n",
    "obs_for_prediction = data_scaler.transform(obs_for_prediction_unscaled)\n",
    "knn_classifier_deployment.predict(obs_for_prediction)   #prediction for unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "#generating data\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
    "\n",
    "#predict cluster labels\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "#plot result\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=1.0);\n",
    "\n",
    "#spectral clustering using nearest neighbours\n",
    "from sklearn.cluster import SpectralClustering\n",
    "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n",
    "                           assign_labels='kmeans')\n",
    "labels = model.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colour compression using k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"ggplot\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_sample_image\n",
    "flower = load_sample_image(\"flower.jpg\")\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.axes(xticks=[], yticks=[])\n",
    "ax.imshow(flower);\n",
    "\n",
    "#reshape 427 x 640 x 3 matrix to 2D and scaled 0 -> 1\n",
    "data = flower / 255.0 # use 0...1 scale\n",
    "data = data.reshape(427 * 640, 3)\n",
    "\n",
    "#cluster colours using minibatchk-means (to save computation time)\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "num_clusters = 10\n",
    "kmeans = MiniBatchKMeans(num_clusters)\n",
    "kmeans.fit(data)\n",
    "predicted_data = kmeans.predict(data)\n",
    "\n",
    "#create new matrix with cluster centres\n",
    "new_colors = kmeans.cluster_centers_[predicted_data]\n",
    "\n",
    "#reshape new matrix\n",
    "flower_recolored = new_colors.reshape(flower.shape)\n",
    "\n",
    "#plot result vs original\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6),\n",
    "                       subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.subplots_adjust(wspace=0.05)\n",
    "ax[0].imshow(flower)\n",
    "ax[0].set_title('Original Image', size=16)\n",
    "ax[1].imshow(flower_recolored)\n",
    "ax[1].set_title('Reduced Color Image', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
