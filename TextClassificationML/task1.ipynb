{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone 1 Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "By Harold Davies\n",
    "\n",
    "Date: 5/05/2024\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* nltk\n",
    "* itertools\n",
    "* os\n",
    "* numpy\n",
    "* nltk.probability\n",
    "\n",
    "## Introduction\n",
    "Some raw data has been procured which will be used to develop an algorithm for making predictions based on similar data. The raw data consists of a file structure containing numerous text files with details of job advertisements including job category, title, id, company and description. In this task, the files will be parsed into data structures which allow for the descriptions to be preprocessed ready to be used to construct vectors for model development. Preprocessing tasks include removing case variation, and removing: single letter words, stop words, words appearing only once in the document set and the 50 words appearing in the most documents. The process involves tokenising the job descriptions, but will result in exporting a text document containing the cleaned job information and another text document with the indexed vocabulary set after preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.probability import *\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the working directory we have a folder called data, itself containing 4 folders: Accounting_Finance, Engineering, Healthcare_Nursing and Sales. Each folder contains a number of text files with the naming convention Job_#####, all text file names are unique. This is a sample of one of the text files from the Accounting_Finance folder: \n",
    "\"Title: FP&A  Blue Chip\n",
    "Webindex: 68802053\n",
    "Company: Hays Senior Finance\n",
    "Description: A market leading retail business is going through rapid growth and, due to this expansion, is looking to add a Financial Planning Analyst to its central team based in central London. This is a fantastic opportunity to join a newly created team...\"\n",
    "\n",
    "Here we are assuming the job_##### information is unimportant, as we have webindexes to use as well and will extract them and discard the job#s. The rest of the information available, including job category, procured from the parent folder name, will be saved in lists. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to extract categories (folder name) and text file contents (info)\n",
    "folder_1 = \"./data/Engineering\"\n",
    "folder_2 = \"./data/Accounting_Finance\"\n",
    "folder_3 = \"./data/Healthcare_Nursing\"\n",
    "folder_4 = \"./data/Sales\"\n",
    "folders = [folder_1, folder_2, folder_3, folder_4]\n",
    "\n",
    "#lists for all job details\n",
    "job_info = []                   #raw text\n",
    "job_titles = []                 #Title\n",
    "job_ids = []                    #Webindex\n",
    "job_companies = []              #Company\n",
    "job_descriptions = []           #Description\n",
    "job_categories = []             #Category\n",
    "\n",
    "#extract category and info\n",
    "for folder in folders:\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            job_categories.append(folder[7:])\n",
    "            path = os.path.join(folder,filename)\n",
    "            with open(path,\"r\",encoding = 'unicode_escape') as f:\n",
    "                job_info.append(f.read()) # read the file into a string, and append it to the job_info list\n",
    "                f.close()\n",
    "\n",
    "#code to split info into job title, index, company (if present) and description\n",
    "for job in job_info:\n",
    "    job = job.split('\\n')\n",
    "    job_titles.append(job[0][7:])\n",
    "    job_ids.append(job[1][10:])\n",
    "    if job[2].startswith('Company'):\n",
    "        job_companies.append(job[2][9:])\n",
    "        job_descriptions.append(job[3][13:])\n",
    "    else:\n",
    "        job_companies.append(None)\n",
    "        job_descriptions.append(job[2][13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Auto Electrician\n",
      "Webindex: 69551242\n",
      "Company: BEVAN GROUP\n",
      "Description: All aspects of electrical installation work relating to Commercial vehicle bodybuilding. Work includes fitting of electrical ancillary systems such as vehicle lighting , reverse cameras and weighing equipment etc\n",
      "Category: Engineering\n"
     ]
    }
   ],
   "source": [
    "#code to check file contents have been extracted correctly\n",
    "ind = 130\n",
    "#print(f\"Info: {job_info[ind]}\")                # Info\n",
    "print(f\"Title: {job_titles[ind]}\")              # Title\n",
    "print(f\"Webindex: {job_ids[ind]}\")              # Webindex\n",
    "print(f\"Company: {job_companies[ind]}\")         # Company\n",
    "print(f\"Description: {job_descriptions[ind]}\")  # Description\n",
    "print(f\"Category: {job_categories[ind]}\")       # Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Job Titles: 776\n",
      "Number of Webindices: 776\n",
      "Number of Companies: 776\n",
      "Number of Descriptions: 776\n",
      "Number of Categories: 776\n"
     ]
    }
   ],
   "source": [
    "#check how many job listings we have, and that the lists all have the same length.\n",
    "print(f\"Number of Job Titles: {len(job_titles)}\")\n",
    "print(f\"Number of Webindices: {len(job_ids)}\")\n",
    "print(f\"Number of Companies: {len(job_companies)}\")\n",
    "print(f\"Number of Descriptions: {len(job_descriptions)}\")\n",
    "print(f\"Number of Categories: {len(job_categories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "In this section I will process the descriptions as follows: change all to lower case, tokenize, remove words with length < 2, remove stop words, remove words that only appear once in all desciptions, remove the 50 words which appear in the most descriptions, save them to file in post-processed form and also save the post-processing vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeData(text):\n",
    "    \"\"\"\n",
    "        This function tokenizes a raw text string.\n",
    "    \"\"\"\n",
    "    #change text to lower case\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    #regex expression provided in assignment spec\n",
    "    exp = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "\n",
    "    #set tokenizer using regex expression\n",
    "    tokenizer = nltk.RegexpTokenizer(exp)\n",
    "\n",
    "    #apply tokenizer to text\n",
    "    tokenised_job = tokenizer.tokenize(text_lower)\n",
    "\n",
    "    #return tokenized text\n",
    "    return tokenised_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the function to tokenize job descriptions\n",
    "tokenized_jobs = [tokenizeData(job_desc) for job_desc in job_descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'electrical',\n",
       " 'installation',\n",
       " 'work',\n",
       " 'relating',\n",
       " 'to',\n",
       " 'commercial',\n",
       " 'vehicle',\n",
       " 'bodybuilding',\n",
       " 'work',\n",
       " 'includes',\n",
       " 'fitting',\n",
       " 'of',\n",
       " 'electrical',\n",
       " 'ancillary',\n",
       " 'systems',\n",
       " 'such',\n",
       " 'as',\n",
       " 'vehicle',\n",
       " 'lighting',\n",
       " 'reverse',\n",
       " 'cameras',\n",
       " 'and',\n",
       " 'weighing',\n",
       " 'equipment',\n",
       " 'etc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check tokenization against example from above\n",
    "tokenized_jobs[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_print(tokenised_text):\n",
    "    \"\"\"\n",
    "    Function to print stats of a list of tokenized text\n",
    "    \"\"\"\n",
    "    words = list(chain.from_iterable(tokenised_text))             #put tokens in single list\n",
    "    vocab = set(words)                                            #convert list to a set\n",
    "    lexical_diversity = len(vocab)/len(words)                     #calc lexical diversity\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    print(f\"Total number of tokens: {len(words)}\")\n",
    "    print(f\"Lexical diversity: {lexical_diversity}\")\n",
    "    print(f\"Total number of articles: {len(tokenised_text)}\")\n",
    "    \n",
    "    lens = [len(text) for text in tokenised_text]                 #compile list of text lengths\n",
    "    print(f\"Average document length: {np.mean(lens)}\")\n",
    "    print(f\"Maximum document length: {np.max(lens)}\")\n",
    "    print(f\"Minimum document length: {np.min(lens)}\")\n",
    "    print(f\"Standard deviation of document length: {np.std(lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9834\n",
      "Total number of tokens: 186952\n",
      "Lexical diversity: 0.052601737344345076\n",
      "Total number of articles: 776\n",
      "Average document length: 240.91752577319588\n",
      "Maximum document length: 815\n",
      "Minimum document length: 13\n",
      "Standard deviation of document length: 124.97750685071483\n"
     ]
    }
   ],
   "source": [
    "#let's see the stats\n",
    "stats_print(tokenized_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words with length of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we remove 26 single letter words from the vocabulary using the revomeShortWords function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for removing short words from a list\n",
    "def removeShortWords(text, max_length_to_remove):\n",
    "    return [w for w in text if len(w) > max_length_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words with length < 2\n",
    "threshold = 1\n",
    "tokenized_jobs = [removeShortWords(text, threshold) for text in tokenized_jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9808\n",
      "Total number of tokens: 180913\n",
      "Lexical diversity: 0.05421390392066905\n",
      "Total number of articles: 776\n",
      "Average document length: 233.13530927835052\n",
      "Maximum document length: 795\n",
      "Minimum document length: 13\n",
      "Standard deviation of document length: 121.6048654015839\n"
     ]
    }
   ],
   "source": [
    "#let's see the stats again\n",
    "stats_print(tokenized_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we remove an additional 134 stop words from the vocabulary using removeWords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #extract stop words from nltk into list variable\n",
    "# nltk.download('stopwords')\n",
    "# stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract stop words from file into list variable\n",
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWords(text, list_words_to_remove):\n",
    "    \"\"\"\n",
    "    Function to remove a list of words from a list of words\n",
    "    \"\"\"\n",
    "    return [w for w in text if w not in list_words_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "tokenized_jobs = [removeWords(text, stopwords) for text in tokenized_jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5169\n",
      "Total number of tokens: 82786\n",
      "Lexical diversity: 0.06243809339743435\n",
      "Total number of articles: 776\n",
      "Average document length: 106.68298969072166\n",
      "Maximum document length: 402\n",
      "Minimum document length: 7\n",
      "Standard deviation of document length: 59.07955949247282\n"
     ]
    }
   ],
   "source": [
    "#let's see the stats again\n",
    "stats_print(tokenized_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words appearing only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to compile the term frequency to find words which appear only once. Then we use the RemoveWords function defined above to remove those words. This resulted in 4186 additional words being removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all words in one list and generate the term frequency distribution\n",
    "words = list(chain.from_iterable(tokenized_jobs))\n",
    "term_fd = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract list of words appearing only once\n",
    "words_appearing_once = [word for word, freq in term_fd.items() if freq == 1]\n",
    "words_appearing_once[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many words appear only once?\n",
    "len(words_appearing_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words appearing once\n",
    "tokenized_jobs = [removeWords(text, words_appearing_once) for text in tokenized_jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5169\n",
      "Total number of tokens: 82786\n",
      "Lexical diversity: 0.06243809339743435\n",
      "Total number of articles: 776\n",
      "Average document length: 106.68298969072166\n",
      "Maximum document length: 402\n",
      "Minimum document length: 7\n",
      "Standard deviation of document length: 59.07955949247282\n"
     ]
    }
   ],
   "source": [
    "#let's see the stats again\n",
    "stats_print(tokenized_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove top 50 words in most descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will find the 50 words which appear in the most documents, then we will remove them from our descriptions using removeWords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('originally', 191),\n",
       " ('jobseeking', 191),\n",
       " ('include', 187),\n",
       " ('clients', 187),\n",
       " ('good', 187),\n",
       " ('essential', 186),\n",
       " ('information', 184),\n",
       " ('customer', 182)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate document frequency distribution\n",
    "word_document_freq = list(chain.from_iterable([set(text) for text in tokenized_jobs]))\n",
    "docfd = FreqDist(word_document_freq)\n",
    "docfd.most_common(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a library of the most 50 most common and extract a list of the words\n",
    "lib_of_most_common = docfd.most_common(50)\n",
    "most_common_words = []\n",
    "for pair in docfd.most_common(50):\n",
    "    most_common_words.append(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the most common words\n",
    "tokenized_jobs = [removeWords(text, most_common_words) for text in tokenized_jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5119\n",
      "Total number of tokens: 71421\n",
      "Lexical diversity: 0.07167359740132453\n",
      "Total number of articles: 776\n",
      "Average document length: 92.03737113402062\n",
      "Maximum document length: 351\n",
      "Minimum document length: 6\n",
      "Standard deviation of document length: 52.3153837469842\n"
     ]
    }
   ],
   "source": [
    "#let's see the stats one last time\n",
    "stats_print(tokenized_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "We save the data, including cleaned job descriptions and job categories, into a text file jobs_clean.txt. Note that info for all jobs are in the same text file, seperated with new lines, which will be easy to parse in the next task thanks to the consistent formatting, with missing company names populated with \"None\". We also save the vocabulary in a text file vocab.txt, in the format \"\\<word\\>:\\<index\\>\\n\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text file for use in milestone 2\n",
    "output_file = open(\"./jobs_clean.txt\", \"w\")\n",
    "for a, b, c, d, e, f in zip(job_titles, job_ids, job_companies, job_descriptions, tokenized_jobs, job_categories):\n",
    "    output_file.write(a + '\\n' + b + '\\n' + str(c) + '\\n' + d + '\\n' + ' '.join(e) + '\\n' + f + '\\n')\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get set of words from clean descriptions\n",
    "vocab = set(list(chain.from_iterable([text for text in tokenized_jobs])))\n",
    "\n",
    "#convert to sorted list\n",
    "vocab = sorted(list(vocab))\n",
    "\n",
    "#save clean vocab to output file\n",
    "output_file = open(\"./vocab.txt\", \"w\")\n",
    "for i in range(0, len(vocab)):\n",
    "    output_file.write(\"{}:{}\\n\".format(vocab[i], i))\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Preprocessing tasks included removing case variation, then 26 single letter words, 404 stop words, 4186 words appearing only once in the document set and the 50 words appearing in the most documents were removed. In total 105,747 occurances of 4,666 words were removed. The lexical diversity of the document set was changed from 0.0526 to 0.064. The process involved tokenising the job descriptions, with the result being an exported text document containing the cleaned job information and another text document with the indexed vocabulary set after preprocessing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
