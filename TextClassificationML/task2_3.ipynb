{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone 1 Natural Language Processing\n",
    "## Task 2 & 3\n",
    "By Harold Davies\n",
    "\n",
    "Date: 5/05/2024\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* sklearn\n",
    "* logging\n",
    "* numpy\n",
    "* gensim\n",
    "* nltk\n",
    "* scipy\n",
    "* matplotlib\n",
    "\n",
    "## Introduction\n",
    "After the job descriptions had been preprocessed in Task 1, Task 2 involved generating feature sets from the descriptions using 3 different methods, and Task 3 followed on with using the feature sets, complimented with some additional details to perform classification of the jobs ads. The methods used for generating feature sets were a simple count vector of words occuring in each description, and TF-IDF weighted and unweighted vectors generated using a Word2Vec model trained on the Google News 300 dataset. Once the weighted and unweighted vectors were generated, we took a random 50% sample of the 300 features and reduced their dimensionality down to 2D using t-SNE, then made a scatterplot of the resulting 2 features to visualise the distribution of target features among the vectors. Each feature set was used to classify job advertisements using a Support Vector Machines model and evaluated using a 5-fold Stratified k-Fold Evaluation cross valuation method. Finally the job titles were tokenized, and vectors were built using the aforementioned pretrained model on just the job titles, and on a combination of job titles and descriptions, and model performance was compared between these vector inputs and that of the job descriptions only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import logging\n",
    "import gensim.downloader as api         #only required for initial model loading\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import nltk\n",
    "from scipy import stats\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where code chunks were required to be reused throughout this project, functions have been defined and are shown below. See the function documentation for further details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcUnweightedVector(tokenized_text, model):\n",
    "    \"\"\"\n",
    "    Function for calculating unweighted vectors based on inputs:\n",
    "        a pretrained model, and \n",
    "        a tokenised document set\n",
    "    function prints the tokens from documents where no words were present \n",
    "    in the pretraining vocabulary for that document\n",
    "    \"\"\"\n",
    "    #generate array to store document vectors\n",
    "    PT_textvecs = np.zeros((len(tokenized_text), model.vector_size))\n",
    "\n",
    "    #for each document\n",
    "    for i, doc in enumerate(tokenized_text):\n",
    "        #valid keys are words in both the document and the model\n",
    "        valid_keys = [term for term in doc if term in model.key_to_index]\n",
    "        #compile matrix of word vectors from the model for each word in the document\n",
    "        try:\n",
    "            docvec = np.vstack([model[term] for term in valid_keys])\n",
    "        except:\n",
    "            print(f\"None of the words from '{doc}' are in the pretrained model\")\n",
    "        #sum columns to obtain document vector based on model\n",
    "        docvec = np.sum(docvec, axis=0)     #possible alternatives to sum such as mean\n",
    "        #save doc vector into row corresponding with document index\n",
    "        PT_textvecs[i,:] = docvec\n",
    "    return PT_textvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareFeatureSets(features_sets, features_set_names, target, random_seed):\n",
    "    \"\"\"\n",
    "    Function for running different feature sets through the selected model\n",
    "    Inputs:\n",
    "        - list of feature sets\n",
    "        - list of feature set names\n",
    "        - target feature\n",
    "        - random seed\n",
    "    Outputs:\n",
    "        - prints mean accuracies and corresponding feature sets\n",
    "        - list of result lists for t-testing\n",
    "    \"\"\"\n",
    "    #list to store lists of each fold's results for use in t-tests\n",
    "    results = []\n",
    "    #evaluation metric\n",
    "    scoring_metric = 'accuracy'\n",
    "    #ML model\n",
    "    clf = SVC() \n",
    "    #cross validation method - stratified k fold\n",
    "    cv_method = StratifiedKFold(n_splits=5,\n",
    "                                shuffle=True, \n",
    "                                random_state=random_seed)\n",
    "    for i, features in enumerate(features_sets):\n",
    "        #cross validation\n",
    "        cv_results_full = cross_val_score(  estimator=clf,\n",
    "                                            X=features,\n",
    "                                            y=target, \n",
    "                                            cv=cv_method, \n",
    "                                            scoring=scoring_metric)\n",
    "        results.append(cv_results_full)\n",
    "        mean_accuracy = cv_results_full.mean().round(3)\n",
    "        print(f\"Accuracy using SVM on {features_set_names[i]}: {mean_accuracy}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeData(text):\n",
    "    \"\"\"\n",
    "        This function tokenizes a raw text string.\n",
    "    \"\"\"\n",
    "    #change text to lower case\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    #regex expression provided in assignment spec\n",
    "    exp = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "\n",
    "    #set tokenizer using regex expression\n",
    "    tokenizer = nltk.RegexpTokenizer(exp)\n",
    "\n",
    "    #apply tokenizer to text\n",
    "    tokenised_job = tokenizer.tokenize(text_lower)\n",
    "\n",
    "    #return tokenized text\n",
    "    return tokenised_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTSNE(labels,features, random_seed): \n",
    "    \"\"\"\n",
    "    Function to plot t-SNE 2 dimensional representations of 50% sample of feature vectors coloured according to their target labels\n",
    "    Inputs:\n",
    "        feature vectors as a numpy array, \n",
    "        list of target labels\n",
    "        random seed\n",
    "    \"\"\"\n",
    "    #convert labels to a numpy array (if it is a list)\n",
    "    if isinstance(labels, list):\n",
    "        labels = np.array(labels)\n",
    "    targets = sorted(np.unique(labels))\n",
    "    #t-SNE is computationally expensive so take just a 50% sample of features\n",
    "    n_samples = int(len(features) * 0.5)\n",
    "    #set random seed to ensure repeatability\n",
    "    np.random.seed(random_seed)\n",
    "    #select indices of features to use in t-SNE\n",
    "    indices = np.random.choice(range(len(features)), size=n_samples, replace=False)\n",
    "    #reduce selected fearures down to 2 dimensions using t-SNE\n",
    "    plot_features = TSNE(n_components=2, random_state=random_seed).fit_transform(features[indices])\n",
    "    #set colors for each target value\n",
    "    colors = ['blue', 'green', 'yellow', 'orange']\n",
    "    #for each unique target value plot observations' 2 features\n",
    "    for i in range(0,len(targets)):\n",
    "        points = plot_features[labels[indices] == targets[i]]\n",
    "        plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[i], label=targets[i])\n",
    "    plt.title(\"2D Feature vector for each document\",\n",
    "              fontdict=dict(fontsize=15))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed\n",
    "rand = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Job Advertisement Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feature sets can be generated, the preprocessed and tokenized data from Task 1 needs to be imported into the notebook. Next, job descriptions are re-tokenized and count and TF-IDF vectors are generated. The Word2Vec word embedding model was used for this project, pretrained on the Google News 300 dataset. Once the pretrained model has been downloaded, it can be saved and loaded from file in future, however, for the sake of this project, the saving and loading code has been commented out. Once the model is loaded we can use it to generate weighted (by TF-IDF) and unweighted vectors for the job descriptions using the model. We took a random 50% sample of the 300 features in each vector generated using the pretrained model, reduced their dimensionality down to 2D using t-SNE, then made scatterplots of the resulting 2 features to visualise the distribution of target features among the vectors. There were some distinct clustering of categories, however it was also evident that sales job ads were associated with a particularly noisy set of data, which would be difficult to differentiate from other ads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists for all job details\n",
    "job_titles = []                 #Title\n",
    "job_ids = []                    #Webindex\n",
    "job_companies = []              #Company\n",
    "job_descriptions = []           #Description\n",
    "job_categories = []             #Category\n",
    "\n",
    "#open the cleaned text file\n",
    "with open('jobs_clean.txt',\"r\",encoding = 'unicode_escape') as f:\n",
    "    #for each line, save it to a list according to its index\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        if i % 6 == 0:\n",
    "            job_titles.append(line)\n",
    "        elif i % 6 == 1:\n",
    "            job_ids.append(int(line))\n",
    "        elif i % 6 == 2:\n",
    "            job_companies.append(line)\n",
    "        elif i % 6 == 4:\n",
    "            job_descriptions.append(line)\n",
    "        elif i % 6 == 5:\n",
    "            job_categories.append(line)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to check file contents have been extracted correctly\n",
    "ind = 400\n",
    "print(f\"Title: {job_titles[ind]}\")              # Title\n",
    "print(f\"Webindex: {job_ids[ind]}\")              # Webindex\n",
    "print(f\"Company: {job_companies[ind]}\")         # Company\n",
    "print(f\"Description: {job_descriptions[ind]}\")  # Description\n",
    "print(f\"Category: {job_categories[ind]}\")       # Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract vobac from file\n",
    "vocab = []\n",
    "with open('vocab.txt','r',encoding = 'unicode_escape') as f:\n",
    "    for line in f:\n",
    "        vocab.append(line.split(':')[0])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retokenize job descriptions\n",
    "tokenized_jobs = [job_desc.split(' ') for job_desc in job_descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check tokenization of running example\n",
    "tokenized_jobs[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count and TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate count vector\n",
    "cVectorizer = CountVectorizer(analyzer = \"word\",vocabulary = vocab)\n",
    "count_features = cVectorizer.fit_transform(job_descriptions)\n",
    "print(count_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate TF-IDF vector\n",
    "tVectorizer = TfidfVectorizer(analyzer = \"word\",vocabulary = vocab)\n",
    "tfidf_features = tVectorizer.fit_transform(job_descriptions)\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec word embedding model pretrained on GoogleNews300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure logging format\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word-vectors from genism-data\n",
    "pretrained_w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model so we do not need to load it again\n",
    "pretrained_w2v_model.save(\"googleNews300_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload pretrained model from file (after model downloaded and saved)\n",
    "# from gensim.models import KeyedVectors\n",
    "# pretrained_w2v_model = KeyedVectors.load('googleNews300_w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unweighted document vectors based on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use function defined at the top of this notebook to generate unweighted vectors\n",
    "PT_w2v_jobvecs = calcUnweightedVector(tokenized_jobs, pretrained_w2v_model)\n",
    "\n",
    "#sanity check the shape and verify I still understand what I am doing! xD\n",
    "PT_w2v_jobvecs.shape\n",
    "\n",
    "# -> matrix of word vectors based on pretrained model for each job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot 2D representation of unweighted vectors for each document coloured according to its target label\n",
    "plotTSNE(job_categories,PT_w2v_jobvecs, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF weighted document vectors based on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create libray to map terms to vocab indices for tf-idf weight lookup\n",
    "term_to_col = {term: col_idx for col_idx, term in enumerate(vocab)}\n",
    "\n",
    "#generate array to store document vectors\n",
    "PT_w2v_jobvecs_weighted = np.zeros((len(tokenized_jobs), pretrained_w2v_model.vector_size))\n",
    "\n",
    "#for each document\n",
    "for i, doc in enumerate(tokenized_jobs):\n",
    "    #valid keys are words in both the document and the model\n",
    "    valid_keys = [term for term in doc if term in pretrained_w2v_model.key_to_index]\n",
    "    #retrieve tf-idf weights for valid words in the document\n",
    "    tf_weights = [tfidf_features[i, term_to_col.get(term, 0)] for term in valid_keys]\n",
    "    #satiny check to ensure data consistency\n",
    "    assert len(valid_keys) == len(tf_weights)\n",
    "    #compile model word vectors weighted by tfidf for each word in the document\n",
    "    weighted = [pretrained_w2v_model[term] * w for term, w in zip(valid_keys, tf_weights)]\n",
    "    #compile matrix of model word vectors\n",
    "    docvec = np.vstack(weighted)\n",
    "    #sum columns to obtain document vector based on model weighted by tfidf vector\n",
    "    docvec = np.sum(docvec, axis=0)     #possible alternatives to sum such as mean \n",
    "    #save doc vector into row corresponding with document index\n",
    "    PT_w2v_jobvecs_weighted[i,:] = docvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix of weighted word vectors based on pretrained model for each job description\n",
    "PT_w2v_jobvecs_weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot 2D representation of weighted vectors for each document coloured according to its target label\n",
    "plotTSNE(job_categories,PT_w2v_jobvecs_weighted, rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the count vector representation as per project spectification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #number of job descriptions\n",
    "# num = count_features.shape[0]\n",
    "# #text file to output count vector\n",
    "# output_file = open('count_vectors.txt', 'w') # creates a txt file and open to save the vector representation\n",
    "# #for each description\n",
    "# for a_ind in range(0, num):\n",
    "#     output_file.write(\"#{}\".format(job_ids[a_ind]))\n",
    "#     #for each non-zero word count\n",
    "#     for f_ind in count_features[a_ind].nonzero()[1]:\n",
    "#         #get the count\n",
    "#         value = count_features[a_ind][0,f_ind]\n",
    "#         #write the word index and count to the file\n",
    "#         output_file.write(\",{}:{}\".format(f_ind,value))\n",
    "#     #insert new line after last word index:count combination\n",
    "#     output_file.write('\\n')\n",
    "# output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Job Advertisement Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have different sets of vectors built from the job descriptions, we can compare their performance. We will also build some sets of vectors for combinations of job title and job description to see how much our model is impacted by additional information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Language model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define list of feature sets to compare\n",
    "features_sets = [count_features, PT_w2v_jobvecs, PT_w2v_jobvecs_weighted]\n",
    "#define list of feature set names for the print statements\n",
    "features_set_names = ['count vectors', 'unweighted pretrained vectors', 'tfidf weighted pretrained vectors']\n",
    "#save the results and compare the various input vector performances\n",
    "results = compareFeatureSets(features_sets, features_set_names, job_categories, rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate p-values for signifance of difference in model results\n",
    "print(stats.ttest_rel(results[1], results[0]).pvalue.round(3))\n",
    "print(stats.ttest_rel(results[1], results[2]).pvalue.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Does more information provide higher accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors for only job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the tokenize function to tokenize the job titles and check against out runnign example\n",
    "tokenized_titles = [tokenizeData(job_title) for job_title in job_titles]\n",
    "tokenized_titles[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use our function to calculate the unweighted vectors and note some irrelevant job titles\n",
    "PT_w2v_job_descvecs = calcUnweightedVector(tokenized_titles, pretrained_w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check we have the desired vector matrix shape\n",
    "PT_w2v_job_descvecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors for only job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#already done!\n",
    "PT_w2v_jobvecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors for job titles and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate list for tokenized titles + descriptions\n",
    "tokenized_title_desc = []\n",
    "\n",
    "#for each combo of title and description, smack 'em together and whack 'em on the list\n",
    "for title, desc in zip(tokenized_titles, tokenized_jobs):\n",
    "    tokenized_title_desc.append(title + desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check our favorite auto 'leccy job ad\n",
    "tokenized_title_desc[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the vectors to generate our new feature set\n",
    "PT_w2v_job_title_descvecs = calcUnweightedVector(tokenized_title_desc, pretrained_w2v_model)\n",
    "PT_w2v_job_title_descvecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define list of feature sets to compare\n",
    "features_sets = [PT_w2v_job_descvecs, PT_w2v_jobvecs, PT_w2v_job_title_descvecs]\n",
    "#define list of feature set names for the print statements\n",
    "features_set_names = ['pretrained vectors using titles only',\n",
    "                      'pretrained vectors using descriptions only',\n",
    "                      'pretrained vectors using titles and descriptions']\n",
    "#save the results and compare the various input vector performances\n",
    "results = compareFeatureSets(features_sets, features_set_names, job_categories, rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate p-values for signifance of difference in model results\n",
    "print(stats.ttest_rel(results[2], results[0]).pvalue.round(3))\n",
    "print(stats.ttest_rel(results[2], results[1]).pvalue.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train SVM on title and description weighted vector and save model for project milestone 2\n",
    "import joblib\n",
    "model = SVC()\n",
    "model.fit(PT_w2v_job_title_descvecs, job_categories)\n",
    "joblib.dump(model, 'best_svm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "After the job descriptions had been preprocessed in Task 1, Task 2 involved generating feature sets from the descriptions using 3 different methods, and Task 3 followed on with using the feature sets, complimented with some additional details to perform classification of the jobs ads. The methods used for generating feature sets were a simple count vector of words occuring in each description, and TF-IDF weighted and unweighted vectors generated using a Word2Vec model trained on the Google News 300 dataset. Once the weighted and unweighted vectors were generated, we plotted 2D representations of the feature sets to visualise the distribution of target features among the vectors, and although there were some distinct clustering of categories, it was evident that sales job ads were associated with a particularly noisy set of data, which would be difficult to differentiate from other ads. Each feature set was used to classify job advertisements using a Support Vector Machines model and evaluated using a 5-fold Stratified k-Fold Evaluation cross valuation method. It was found that the unweighted pretrained vectors had the highest accuracy with 86.6% of jobs being correctly classified, however it was also found that the results were not statistically significantly better than the results using other feature sets. Finally the job titles were tokenized, and vectors were built using the aforementioned pretrained model on just the job titles, and on a combination of job titles and descriptions, and model performance was compared between these vector inputs and that of the job descriptions only. It was found that titles and descriptions in combination provided the highest accuracy followed by only descriptions, however again, the difference in accuracy was less than 2% and found to be not statistically significant. Some recommendations for further analysis would be to leverage stemming and lemmitisation in the preprocessing phase to improve the quality of input data, and to use repeated stratified k-fold cross validation to allow for a more reliable estimation of variance between models and input data options, and result in more robust significance testing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
